{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1b2c99",
   "metadata": {},
   "source": [
    "# Steps for Model Improvement:\n",
    "\n",
    "1. Model Order Selection: Experiment with different values for the p, d, and q parameters of the ARIMA model. You can try different combinations of values and use criteria such as AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to select the best order for your model. Grid search or automated model selection algorithms like auto_arima can also be helpful in finding the optimal parameters.\n",
    "\n",
    "2. Data Transformation: Consider transforming the data to improve stationarity. Stationarity is a key assumption for ARIMA models. You can apply techniques like differencing, logarithmic transformation, or seasonal differencing to achieve stationarity in the data.\n",
    "\n",
    "3. Seasonality: If your data exhibits a seasonal pattern, you can consider using a seasonal ARIMA model (SARIMA) instead of a regular ARIMA model. SARIMA models incorporate seasonal components to capture the seasonal patterns in the data. You can experiment with different seasonal orders (P, D, Q, and s parameters) to model the seasonality effectively.\n",
    "\n",
    "4. Outliers: Check for outliers in your data and consider handling them appropriately. Outliers can have a significant impact on the model's performance. You can identify outliers using techniques like visual inspection, statistical tests, or outlier detection algorithms. Handling outliers can involve replacing them with more reasonable values or excluding them from the modeling process.\n",
    "\n",
    "5. Model Evaluation: Use appropriate evaluation metrics to assess the performance of your model. Along with RMSE, consider using other metrics like mean absolute error (MAE), mean absolute percentage error (MAPE), or forecast accuracy measures like MASE (mean absolute scaled error). These metrics provide different perspectives on the accuracy of the model and can help in comparing different models or variations of the same model.\n",
    "\n",
    "6. Model Validation: Perform model validation by testing the model on out-of-sample data. Instead of using a fixed test set, consider using techniques like rolling or expanding window validation to assess the model's performance across different time periods.\n",
    "\n",
    "## 7. Ensemble Models: Explore ensemble techniques, such as combining multiple ARIMA models or combining ARIMA with other forecasting methods like exponential smoothing or machine learning algorithms. Ensemble methods can help improve the overall forecasting accuracy by leveraging the strengths of different models.\n",
    "\n",
    "9. Additional Features: If available, consider incorporating additional relevant features into your modeling process. These could be external factors or predictors that might have an influence on the time series behavior. Feature engineering and feature selection techniques can be applied to identify and include the most informative features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb65e6",
   "metadata": {},
   "source": [
    "### Arima Assumptions\n",
    "\n",
    "Evaluate model assumptions: Assess the assumptions of the ARIMA model, such as the independence and normality of residuals. If any assumptions are violated, you may need to explore alternative models or apply additional transformations to the data.\n",
    "\n",
    "1. Residual Analysis: Examine the residuals of the model to ensure they are independent, normally distributed, and have constant variance. You can plot the residuals over time, check for patterns or trends, and look for any significant deviations from randomness. You can also create a histogram or a Q-Q plot to assess the normality of the residuals.\n",
    "\n",
    "2. Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) Analysis: Plot the ACF and PACF of the residuals to identify any significant autocorrelation beyond what is captured by the model. Significant autocorrelation may indicate that the model is not capturing all the relevant information in the data.\n",
    "\n",
    "3. Ljung-Box Test: Perform the Ljung-Box test or the Box-Pierce test on the residuals to test for the presence of autocorrelation. These tests evaluate whether a group of autocorrelations in the residuals is significantly different from zero. A low p-value in these tests suggests the presence of autocorrelation.\n",
    "\n",
    "4. Normality Tests: Apply statistical tests for normality, such as the Shapiro-Wilk test or the Anderson-Darling test, to assess whether the residuals are normally distributed. A high p-value indicates that the residuals can be assumed to follow a normal distribution.\n",
    "\n",
    "5. Homoscedasticity Tests: Check for constant variance in the residuals using tests like the Breusch-Pagan test or the White test. These tests assess whether there is heteroscedasticity (varying levels of variance) in the residuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b0f55",
   "metadata": {},
   "source": [
    "### Further investigate the non-stationarity:\n",
    "\n",
    "1. Visualize the time series: Plot the time series data to visually inspect any trends, seasonality, or other patterns in the data. This can be done using line plots, scatter plots, or other relevant visualization techniques.\n",
    "\n",
    "2. Check for stationarity: Use statistical tests to assess the stationarity of the time series. Common tests include the Augmented Dickey-Fuller (ADF) test and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. These tests can help determine if the data is stationary or if differencing is needed.\n",
    "\n",
    "3. Apply transformations: If the data is found to be non-stationary, you can apply transformations to make it stationary. Some common transformations include differencing, logarithmic transformation, and seasonal differencing. Experiment with different transformations and assess their impact on stationarity.\n",
    "\n",
    "4. Examine autocorrelation and partial autocorrelation: Plot the autocorrelation function (ACF) and partial autocorrelation function (PACF) to identify any significant lags in the data. This can provide insights into the order of autoregressive (AR) and moving average (MA) terms for the SARIMAX model.\n",
    "\n",
    "5. Model selection: Once the data is made stationary, you can use model selection techniques such as AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to determine the optimal order of the SARIMAX model. Consider different combinations of AR, MA, seasonal AR, and seasonal MA terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e86dd9",
   "metadata": {},
   "source": [
    "# transformation methods\n",
    "\n",
    "- None, Differencing, Logarithmic, Box-Cox, Seasonal Differencing, Power Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69889e2",
   "metadata": {},
   "source": [
    "# Holt Winter\n",
    "\n",
    "The Holt-Winters method is a popular forecasting technique for time-series data. It can handle trends and seasonality, which makes it very useful for forecasting data patterns that change over time.\n",
    "\n",
    "The method is actually composed of three different forecasting methods:\n",
    "\n",
    "1. **Simple Exponential Smoothing (SES):** This method is suitable for forecasting data with no clear trend or seasonal pattern. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past (the smallest weights are associated with the oldest observations).\n",
    "\n",
    "2. **Holt's Linear Exponential Smoothing (LES):** This method extends SES by adding a component to handle trends in the data. It's suitable for data with a trend but no seasonality.\n",
    "\n",
    "3. **Holt-Winters' Exponential Smoothing (HWES):** This method extends Holt's Linear Exponential Smoothing by adding a component to handle seasonality. It's suitable for data with both a trend and a seasonal pattern.\n",
    "\n",
    "In summary, Holt-Winters is a way to model and predict the future values of a time series that can handle trends (either additive or multiplicative) and seasonality (again, either additive or multiplicative). It does this by applying exponential smoothing to the observations, trend, and seasonal components of a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3409a",
   "metadata": {},
   "source": [
    "### Amape\n",
    "\n",
    "If the cost of over-prediction is greater than the cost of under-prediction, it would be beneficial to use a custom loss function that incorporates different weights for over-prediction and under-prediction. This function could be designed to penalize over-prediction more heavily.\n",
    "\n",
    "One such method is asymmetric mean absolute percentage error (AMAPE), where you assign different weights to the over-estimation and under-estimation. The formula for AMAPE can be represented as:\n",
    "\n",
    "AMAPE = 1/n Σ [actual > predicted] * w1 * |actual - predicted| / actual + [actual ≤ predicted] * w2 * |actual - predicted| / actual\n",
    "\n",
    "For instance, if overprediction_weight is set to 1.5 and underprediction_weight to 1, this means that over-predictions will be penalized 1.5 times more than under-predictions. So, if you have an over-prediction of 10%, it will contribute 15% (because 10% * 1.5 = 15%) to the final error measure. On the contrary, an under-prediction of 10% will contribute 10% to the final error measure. The error measure returned by the function is the mean of these weighted percentage errors.\n",
    "\n",
    "### Descriptions\n",
    "\n",
    "RMSE squares these differences, averages them, and then takes the square root. This approach gives more weight to larger differences due to the squaring operation. Therefore, RMSE is more sensitive to outliers than AMAPE.\n",
    "\n",
    "AMAPE, on the other hand, computes the absolute percentage differences, and then averages them. The asymmetry comes into play by applying different weights to overpredictions and underpredictions.\n",
    "\n",
    "If your concern is that your predictions are consistently higher than your actual values (overprediction), and you want to penalize overpredictions more heavily, you should apply a higher weight to overpredictions. So, in your asymmetric_mean_absolute_percentage_error function, you would set overprediction_weight to a value greater than 1, and underprediction_weight to 1.\n",
    "\n",
    "\n",
    "\n",
    "RMSE:\n",
    "\n",
    "RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable.\n",
    "MAE:\n",
    "\n",
    "MAE is more robust to outliers than RMSE. If there are outliers in the data, we should consider using MAE as it may lead to better results.\n",
    "\n",
    "Root Mean Squared Error (RMSE): RMSE is a commonly used metric in regression problems. The squaring operation penalizes larger errors more than smaller ones, meaning that RMSE will be more sensitive to outliers. RMSE can be useful if large errors are particularly undesirable.\n",
    "\n",
    "Mean Absolute Error (MAE): MAE is the average of the absolute differences between the predicted and actual values. It gives an idea of how wrong the predictions were. The measure gives equal weight to all errors, no matter how big or small they are. If all errors are equally important to you, MAE could be a good choice.\n",
    "\n",
    "Mean Absolute Percentage Error (MAPE): MAPE expresses the absolute error as a percentage of the actual value. This is useful for comparing errors across different scales or units. If you want to understand the size of your errors relative to the actual values, you might use MAPE.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "The Models:\n",
    "We have run different models on our data, each using a different transformation. Each of these models attempts to predict the number of taxi rides in NYC. The different methods we've used include TNC, basinhopping, least_squares, and Powell, each providing a different approach to fit the model to the data.\n",
    "\n",
    "MAPE:\n",
    "MAPE stands for Mean Absolute Percentage Error. It's a measure that tells us how much our predictions, on average, deviate from the actual numbers in terms of percentage. So, if the MAPE is 1.3%, this means that, on average, our predictions are off by about 1.3% from the actual number of taxi rides. The smaller this percentage is, the closer our predictions are to the real numbers, which is what we want.\n",
    "\n",
    "AMAPE:\n",
    "AMAPE stands for Asymmetric Mean Absolute Percentage Error. It's a variation of MAPE, but with a twist: we don't consider all mistakes as equal. We've decided that over-predicting the number of taxi rides (predicting more than there actually are) is twice as bad as under-predicting. Hence, we have put a heavier penalty on over-predictions. This is reflected in the values you see in the AMAPE row.\n",
    "\n",
    "The Results:\n",
    "Looking at the results, the model using the TNC method with a logarithmic transformation gives the best performance in terms of both MAPE and AMAPE. This means it's closest to the actual values, and it balances over- and under-prediction in a way that suits our needs (given our penalty for over-prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cff0fe",
   "metadata": {},
   "source": [
    "The skewness and kurtosis of your residuals indicate that they are not normally distributed. A perfectly normal distribution has a skewness of 0 (indicating symmetry) and a kurtosis of 3 (reflecting the peakiness or flatness of the distribution). In your case, the skewness is -4.43 and the kurtosis is 29.68. These values indicate a distribution that is negatively skewed (more values to the right of the mean) and leptokurtic (with more extreme values or outliers than a normal distribution).\n",
    "\n",
    "The Shapiro-Wilk test (a common test for normality) confirms this observation. With a test statistic of 0.615 and a p-value of 0.000, the null hypothesis of the residuals following a normal distribution is rejected. A low p-value (less than 0.05) indicates strong evidence against the null hypothesis.\n",
    "\n",
    "Overall, your residuals do not follow a normal distribution, which is an assumption for many time series forecasting models. However, this does not necessarily invalidate your model. Some models can handle non-normality, and sometimes the non-normality arises from factors like outliers that can be addressed separately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685191d",
   "metadata": {},
   "source": [
    "### Autocorrelation of Residuals:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462c7ee",
   "metadata": {},
   "source": [
    "Autocorrelation, also known as serial correlation, refers to the degree of correlation between the values of a variable with its own lagged values in a time series dataset. In simpler terms, it is the similarity between observations as a function of the time lag between them.\n",
    "\n",
    "For instance, if a time series data has a positive autocorrelation, it means that if a higher-than-average value occurs, then the next value will also likely be higher than average. Conversely, if the autocorrelation is negative, then a higher-than-average value will likely be followed by a lower-than-average value, and vice versa.\n",
    "\n",
    "From the autocorrelation function (ACF) and partial autocorrelation function (PACF), it appears there is some autocorrelation present at various lags. \n",
    "\n",
    "This is indicated by the fact that several lags have ACF and PACF values that are significantly different from 0. This would suggest that there is some information in the residuals that is not being captured by the current model.\n",
    "\n",
    "An autocorrelation near zero for all lag values would imply that the model does not leave any linear trends unaccounted for in the residuals. However, it's not unusual to see some significant autocorrelations at the first few lags in the residuals of a time series forecasting model. These might be worth investigating further.\n",
    "\n",
    "In summary, the model could potentially be improved by addressing the non-normality and autocorrelation in the residuals. This might involve transformations of the data, handling of outliers, or a different model specification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c513b7",
   "metadata": {},
   "source": [
    "### Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cf944",
   "metadata": {},
   "source": [
    "The relevance of conducting the ADF and KPSS tests on the residuals is to assess their stationarity. \n",
    "\n",
    "Stationarity is an important assumption in many time series models, including SARIMA and Holt-Winters, as it ensures that the statistical properties of the data remain consistent over time.\n",
    "\n",
    "In this case, the ADF test suggests that the residuals are stationary, indicating that they do not have a unit root and exhibit a stable pattern. This is desirable for modeling purposes, as it allows for the reliable estimation of model parameters and accurate forecasting.\n",
    "\n",
    "On the other hand, the KPSS test suggests that the residuals are not stationary, which implies the presence of a trend or structural change in the data. This discrepancy between the ADF and KPSS tests could indicate some underlying complexity in the data that may require further exploration and consideration in the modeling process.\n",
    "\n",
    "Based on these results, you can proceed with modeling using the assumption of stationarity given by the ADF test. However, it's also important to be mindful of the potential non-stationarity indicated by the KPSS test. You may want to investigate the reasons behind this discrepancy, such as the presence of long-term trends or structural breaks, and consider additional techniques or models that can handle such patterns if needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d7b27",
   "metadata": {},
   "source": [
    "- **Yellow:** The SARIMAX(3,0,3) model provides the best fit, with an RMSE of 4731.296. This suggests that 3 past terms and 3 past error terms are useful for predicting the 'Trips Per Day'.\n",
    "\n",
    "- **p - Past Terms (Autoregressive terms):** Imagine trying to predict a stock's price tomorrow. You'd probably look at its price today, yesterday, and maybe the day before that. The autoregressive part of our model does exactly this: it uses the stock's price from the past few days to predict its price tomorrow.\n",
    "\n",
    "- **d- Levels of Differencing:** Sometimes a stock's price doesn't change in a predictable way, but the change in its price does. For example, maybe a stock tends to go up by $1 each day. If we tried to predict the price directly, we'd have a hard time because the price keeps changing. But if we predict the change in price (the difference), it becomes much easier because the change is the same each day. This is what we mean by differencing: we're predicting the change in price rather than the price itself.\n",
    "\n",
    "- **q - Past Error Terms (Moving Average terms):** Let's continue with our stock prediction. Even with the best model, our predictions will sometimes be off, either too high or too low. These are our errors. The moving average part of our model says: \"Let's not just throw these errors away. If we've been consistently overestimating the stock's price, we should take this into account and lower our predictions a bit.\"\n",
    "\n",
    "In our model, we decide how many days worth of past prices to consider (the 'p' value), how many of our past errors to take into account (the 'q' value), and whether to predict the price directly or the change in price (the 'd' value). By adjusting these parameters, we can make our model match the patterns we see in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0824068",
   "metadata": {},
   "source": [
    "residuals = test - predictions\n",
    "\n",
    "The above code calculates the residuals directly by subtracting the predicted values from the actual values. This is generally used when you've generated predictions on a test set and want to evaluate the residuals of these predictions.\n",
    "\n",
    "residuals = model_fit.resid\n",
    "\n",
    "On the other hand, this line of code retrieves the residuals from a fitted model. These residuals are the differences between the actual values and the predicted values for the data that was used to fit the model (i.e., your training set).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
